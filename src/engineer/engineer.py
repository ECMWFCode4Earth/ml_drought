import numpy as np
import calendar
from collections import defaultdict
from datetime import datetime, date
from pathlib import Path
import pickle
import xarray as xr

from typing import cast, DefaultDict, Dict, List, Optional, Union, Tuple

from ..utils import minus_months


class Engineer:
    """The engineer is responsible for taking all the data from the preprocessors,
    and saving a single training netcdf file to be ingested by machine learning models.

    Training and test sets are defined here, to ensure different machine learning models have
    access to the same data.

    Attributes:
    ----------
    data_folder: Path, default: Path('data')
        The location of the data folder.
    """

    def __init__(self, data_folder: Path = Path('data')) -> None:

        self.data_folder = data_folder

        self.interim_folder = data_folder / 'interim'
        assert self.interim_folder.exists(), \
            f'{data_folder / "interim"} does not exist. Has the preprocesser been run?'

        self.output_folder = data_folder / 'features'
        if not self.output_folder.exists():
            self.output_folder.mkdir()

        self.num_normalization_values: int = 0
        self.normalization_values: DefaultDict[str, Dict[str, np.ndarray]] = defaultdict(dict)

    def engineer(self, test_year: Union[int, List[int]],
                 target_variable: str = 'VHI',
                 pred_months: int = 11,
                 expected_length: Optional[int] = 11
                 ):
        """
        Take all the preprocessed data generated by the preprocessing classes, and turn it
        into a single training file to be ingested by the machine learning models.

        Arguments
        ----------
        test_year: Union[int, List[int]]
            Data to be used for testing. No data earlier than the earliest test year
            will be used for training.
            If a list is passed, a file for each year will be saved.
        target_variable: str = 'VHI'
            The variable to be predicted. Only this variable will be saved in the test
            netcdf files
        pred_months: int = 11
            The amount of months of data to feed as input to the model for it to make its
            prediction
        expected_length: Optional[int] = 11
            The expected length of the x data along its time-dimension. If this is not None
            and an x array has a different time dimension size, the array is ignored. This
            differs from pred_months if the preprocessors are run with a time granularity
            different from 'M'
        """
        data = self._make_dataset()

        if type(test_year) is int:
            test_year = [cast(int, test_year)]

        train_ds = self._train_test_split(data, cast(List, test_year), target_variable,
                                          pred_months, expected_length)

        self._stratify_training_data(train_ds, target_variable, pred_months,
                                     expected_length)

        for var in self.normalization_values.keys():
            self.normalization_values[var]['mean'] /= self.num_normalization_values
            self.normalization_values[var]['std'] /= self.num_normalization_values

        savepath = self.output_folder / 'normalizing_dict.pkl'
        with savepath.open('wb') as f:
            pickle.dump(self.normalization_values, f)

    def _get_preprocessed_files(self) -> List[Path]:
        processed_files = []
        for subfolder in self.interim_folder.iterdir():
            if str(subfolder).endswith('_preprocessed') and subfolder.is_dir():
                processed_files.extend(list(subfolder.glob('*.nc')))
        return processed_files

    def _make_dataset(self) -> xr.Dataset:

        datasets = []
        dims = ['lon', 'lat']
        coords = {}
        for idx, file in enumerate(self._get_preprocessed_files()):
            print(f'Processing {file}')
            datasets.append(xr.open_dataset(file))

            if idx == 0:
                for dim in dims:
                    coords[dim] = datasets[idx][dim].values
            else:
                for dim in dims:
                    assert np.array_equal(datasets[idx][dim].values, coords[dim]), \
                        f'{dim} is different! Was this run using the preprocessor?'

        main_dataset = datasets[0]
        for dataset in datasets[1:]:
            main_dataset = main_dataset.merge(dataset, join='inner')

        return main_dataset

    def _stratify_training_data(self, train_ds: xr.Dataset,
                                target_variable: str,
                                pred_months: int = 11,
                                expected_length: Optional[int] = 11
                                ) -> None:

        min_date = self.get_datetime(train_ds.time.values.min())
        max_date = self.get_datetime(train_ds.time.values.max())

        cur_pred_year, cur_pred_month = max_date.year, max_date.month

        cur_min_date = max_date
        while cur_min_date >= min_date:

            arrays, cur_min_date = self.stratify_xy(train_ds, cur_pred_year,
                                                    target_variable, cur_pred_month,
                                                    pred_months, expected_length)
            if arrays is not None:
                self.calculate_normalization_values(arrays['x'])
                self._save(arrays, year=cur_pred_year, month=cur_pred_month,
                           dataset_type='train')
            cur_pred_year, cur_pred_month = cur_min_date.year, cur_min_date.month

    def _train_test_split(self, ds: xr.Dataset,
                          years: List[int],
                          target_variable: str,
                          pred_months: int = 11,
                          expected_length: Optional[int] = 11,
                          ) -> xr.Dataset:
        years.sort()

        xy_test, min_test_date = self.stratify_xy(ds, years[0], target_variable, 1,
                                                  pred_months, expected_length)

        train_dates = ds.time.values <= np.datetime64(str(min_test_date))
        train_ds = ds.isel(time=train_dates)

        if xy_test is not None:
            self._save(xy_test, year=years[0], month=1,
                       dataset_type='test')

        for year in years:
            for month in range(1, 13):
                if year > years[0] or month > 1:
                    # prevents the initial test set from being recalculated
                    xy_test, _ = self.stratify_xy(ds, year, target_variable, month,
                                                  pred_months, expected_length)
                    if xy_test is not None:
                        self._save(xy_test, year=year, month=month,
                                   dataset_type='test')
        return train_ds

    @staticmethod
    def stratify_xy(ds: xr.Dataset,
                    year: int,
                    target_variable: str,
                    target_month: int,
                    pred_months: int,
                    expected_length: Optional[int],
                    ) -> Tuple[Optional[Dict[str, xr.Dataset]], date]:

        print(f'Generating data for year: {year}, target month: {target_month}')

        max_date = date(year, target_month, calendar.monthrange(year, target_month)[-1])
        mx_year, mx_month, max_train_date = minus_months(year, target_month, diff_months=1)
        _, _, min_date = minus_months(mx_year, mx_month, diff_months=pred_months)

        min_date_np = np.datetime64(str(min_date))
        max_date_np = np.datetime64(str(max_date))
        max_train_date_np = np.datetime64(str(max_train_date))

        print(f'Max date: {str(max_date)}, max input date: {str(max_train_date)}, '
              f'min input date: {str(min_date)}')

        x = ((ds.time.values > min_date_np) & (ds.time.values <= max_train_date_np))
        y = ((ds.time.values > max_train_date_np) & (ds.time.values <= max_date_np))

        if sum(y) != 1:
            print(f'Wrong number of y values! Expected 1, got {sum(y)}; returning None')
            return None, cast(date, max_train_date)

        if expected_length is not None:
            if sum(x) != expected_length:
                print(f'Wrong number of x values! Got {sum(x)} Returning None')

                return None, cast(date, max_train_date)

        x_dataset = ds.isel(time=x)
        y_dataset = ds.isel(time=y)[target_variable].to_dataset(name=target_variable)

        return {'x': x_dataset, 'y': y_dataset}, cast(date, max_train_date)

    @staticmethod
    def get_datetime(time: np.datetime64) -> date:
        return datetime.strptime(time.astype(str)[:10], '%Y-%m-%d').date()

    def _save(self, ds_dict: Dict[str, xr.Dataset], year: int,
              month: int, dataset_type: str) -> None:

        save_folder = self.output_folder / dataset_type
        save_folder.mkdir(exist_ok=True)

        output_location = save_folder / f'{year}_{month}'
        output_location.mkdir(exist_ok=True)

        for x_or_y, output_ds in ds_dict.items():
            output_ds.to_netcdf(output_location / f'{x_or_y}.nc')

    def calculate_normalization_values(self, x_data: xr.Dataset) -> None:

        for var in x_data.data_vars:
            mean = x_data[var].mean(dim=['lat', 'lon']).values
            std = x_data[var].std(dim=['lat', 'lon']).values

            if var in self.normalization_values:
                self.normalization_values[var]['mean'] += mean
                self.normalization_values[var]['std'] += std
            else:
                self.normalization_values[var]['mean'] = mean
                self.normalization_values[var]['std'] = std

        self.num_normalization_values += 1
