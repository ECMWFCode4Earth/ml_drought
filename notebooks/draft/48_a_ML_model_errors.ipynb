{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Model Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/jovyan/ml_drought\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# ignore warnings for now ...\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if Path('.').absolute().parents[1].name == 'ml_drought':\n",
    "    os.chdir(Path('.').absolute().parents[1])\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data/')\n",
    "\n",
    "assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import drop_nans_and_flatten\n",
    "\n",
    "from src.analysis import read_train_data, read_test_data, read_pred_data\n",
    "from src.analysis.evaluation import join_true_pred_da\n",
    "from src.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT =        'one_timestep_forecast' # '2020_04_23:190425_one_timestep_forecast' \n",
    "TRUE_EXPERIMENT =   'one_timestep_forecast'\n",
    "TARGET_VAR =        'discharge_spec'\n",
    "STATIC_DATA_FILE =  'data.nc' # '2020_04_23:112630_data.nc_'\n",
    "DYNAMIC_DATA_FILE = 'data.nc'\n",
    "N_EPOCHS = 100\n",
    "\n",
    "assert (data_dir / f\"models/{EXPERIMENT}\").exists()\n",
    "assert (data_dir / f\"features/{TRUE_EXPERIMENT}\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error: /home/jovyan/ml_drought/data/features/one_timestep_forecast/data.nc is not a valid NetCDF 3 file\n            If this is a NetCDF4 file, you may need to install the\n            netcdf4 library, e.g.,\n\n            $ pip install netcdf4\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/lru_cache.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_to_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: [<function _open_scipy_netcdf at 0x7f8468e164d0>, ('/home/jovyan/ml_drought/data/features/one_timestep_forecast/data.nc',), 'r', (('mmap', None), ('version', 2))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/scipy_.py\u001b[0m in \u001b[0;36m_open_scipy_netcdf\u001b[0;34m(filename, mode, mmap, version)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetcdf_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# netcdf3 message is obscure in this case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/io/netcdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, mmap, version, maskandscale)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'ra'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/scipy/io/netcdf.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    608\u001b[0m             raise TypeError(\"Error: %s is not a valid NetCDF 3 file\" %\n\u001b[0;32m--> 609\u001b[0;31m                             self.filename)\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'version_byte'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'>b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Error: /home/jovyan/ml_drought/data/features/one_timestep_forecast/data.nc is not a valid NetCDF 3 file",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f6aa9ecb1356>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# read in the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/features/{TRUE_EXPERIMENT}/{DYNAMIC_DATA_FILE}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# static_ds = xr.open_dataset(Path(f'data/features/static/data.nc'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_static\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'data/interim/static/data.nc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, group, decode_cf, mask_and_scale, decode_times, autoclose, concat_characters, decode_coords, engine, chunks, lock, cache, drop_variables, backend_kwargs, use_cftime)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mclose_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_decode_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;31m# Ensure source filename always stored in dataset object (GH issue #2550)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/api.py\u001b[0m in \u001b[0;36mmaybe_decode_store\u001b[0;34m(store, lock)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mdecode_coords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_coords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mdrop_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_variables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0muse_cftime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cftime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         )\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/conventions.py\u001b[0m in \u001b[0;36mdecode_cf\u001b[0;34m(obj, concat_characters, mask_and_scale, decode_times, decode_coords, drop_variables, use_cftime)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbstractDataStore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mextra_coords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mfile_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/common.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[1;32m    122\u001b[0m         variables = FrozenDict(\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_decode_variable_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         )\n\u001b[1;32m    125\u001b[0m         \u001b[0mattributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrozenDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/scipy_.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         return FrozenDict(\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/scipy_.py\u001b[0m in \u001b[0;36mds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_store_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mAn\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mfile\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_acquire_with_cache_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneeds_lock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/file_manager.py\u001b[0m in \u001b[0;36m_acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;31m# ensure file doesn't get overriden when opened again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xarray/backends/scipy_.py\u001b[0m in \u001b[0;36m_open_scipy_netcdf\u001b[0;34m(filename, mode, mmap, version)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \"\"\"\n\u001b[1;32m     93\u001b[0m             \u001b[0merrmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Error: /home/jovyan/ml_drought/data/features/one_timestep_forecast/data.nc is not a valid NetCDF 3 file\n            If this is a NetCDF4 file, you may need to install the\n            netcdf4 library, e.g.,\n\n            $ pip install netcdf4\n            "
     ]
    }
   ],
   "source": [
    "# read in the training data\n",
    "ds = xr.open_dataset(Path(f'data/features/{TRUE_EXPERIMENT}/{DYNAMIC_DATA_FILE}'))\n",
    "\n",
    "# static_ds = xr.open_dataset(Path(f'data/features/static/data.nc'))\n",
    "all_static = xr.open_dataset(Path(f'data/interim/static/data.nc'))\n",
    "all_static['station_id'] = all_static['station_id'].astype(int)\n",
    "static = all_static\n",
    "\n",
    "ds['station_id'] = ds['station_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catchment_ids = [int(c) for c in [\"12002\", \"15006\", \"27009\", \"27034\", \"27041\", \"39001\", \"39081\", \"43021\", \"47001\", \"54001\", \"54057\", \"71001\", \"84013\",]]\n",
    "catchment_names = [\"Dee@Park\", \"Tay@Ballathie\", \"Ouse@Skelton\", \"Ure@Kilgram\", \"Derwent@Buttercrambe\", \"Thames@Kingston\", \"Ock@Abingdon\", \"Avon@Knapp\", \"Tamar@Gunnislake\", \"Severn@Bewdley\", \"Severn@Haw\", \"Ribble@Samlesbury\", \"Clyde@Daldowie\"]\n",
    "station_map = dict(zip(catchment_ids, catchment_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read AWS Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([d.name for d in (data_dir/'gcloud').iterdir()])\n",
    "print([d.name for d in (data_dir/'gcloud').glob('*_less_vars/*.csv')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_less_vars = pd.read_csv(data_dir / \"gcloud/lstm_less_vars/results_lstm_less_vars_1307_1717_E015.csv\")\n",
    "lstm_less_vars = pd.read_csv(data_dir / \"gcloud/less_vars_2004_2015/all_lstm_less_vars_2004_1507_1028_results.csv\")\n",
    "lstm_less_vars = lstm_less_vars[[\"station_id\", \"time\", \"obs\", \"sim_E015\"]].rename(columns=dict(sim_E015=\"sim\"))\n",
    "lstm_less_vars[\"time\"] = pd.to_datetime(lstm_less_vars[\"time\"])\n",
    "\n",
    "# ealstm_less_vars = pd.read_csv(data_dir / \"gcloud/ealstm_less_vars/results_ealstm_less_vars_1307_2051_E015.csv\")\n",
    "# ealstm_less_vars = pd.read_csv(data_dir / \"gcloud/less_vars_2004_2015/results_ealstm_less_vars_2004_1507_2033_E007.csv\")\n",
    "ealstm_less_vars = pd.read_csv(data_dir / \"gcloud/less_vars_2004_2015/results_ealstm_less_vars_2004_1707_1424_E011.csv\")\n",
    "\n",
    "\n",
    "# ealstm_less_vars = pd.read_csv(data_dir / \"gcloud/less_vars_2004_2015/results_ealstm_less_vars_2004_1507_2033_E007.csv\")\n",
    "ealstm_less_vars[\"time\"] = pd.to_datetime(ealstm_less_vars[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = lstm_less_vars.set_index([\"station_id\", \"time\"]).to_xarray()\n",
    "ealstm_preds = ealstm_less_vars.set_index([\"station_id\", \"time\"]).to_xarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUSE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = [d for d in (data_dir / \"gcloud/FUSE/Timeseries_SimQ_Best/\").glob(\"*_Best_Qsim.txt\")]\n",
    "\n",
    "if not (data_dir / \"gcloud/ALL_fuse_ds.nc\").exists():\n",
    "    all_dfs = []\n",
    "    for txt in tqdm(all_paths):\n",
    "        df = pd.read_csv(txt, skiprows=3, header=0)\n",
    "        df.columns = [c.rstrip().lstrip() for c in df.columns]\n",
    "        df = df.rename(columns={\"YYYY\": \"year\", \"MM\": \"month\", \"DD\": \"day\"})\n",
    "        df[\"time\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "        station_id = int(str(txt).split(\"/\")[-1].split(\"_\")[0])\n",
    "        df[\"station_id\"] = [station_id for _ in range(len(df))]\n",
    "        df = df.drop([\"year\", \"month\", \"day\", \"HH\"], axis=1).set_index([\"station_id\", \"time\"])\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "    fuse_ds = pd.concat(all_dfs).to_xarray()\n",
    "    fuse_ds.to_netcdf(data_dir / \"gcloud/ALL_fuse_ds.nc\")\n",
    "    \n",
    "else:\n",
    "    fuse_ds = xr.open_dataset(data_dir / \"gcloud/ALL_fuse_ds.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_ds = fuse_ds.sel(time=slice('2004-01-01', '2009-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with observations for stations that exist\n",
    "obs = (\n",
    "    ds.sel(station_id=np.isin(ds[\"station_id\"], fuse_ds[\"station_id\"]), time=fuse_ds[\"time\"])[\"target_var_original\"]\n",
    ").rename(\"obs\")\n",
    "fuse_data = fuse_ds.sel(station_id=obs.station_id).merge(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Stations / Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stations = np.isin(lstm_preds.station_id, fuse_data.station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_preds = lstm_preds.sel(station_id=all_stations, time=fuse_data.time)\n",
    "ealstm_preds = ealstm_preds.sel(station_id=all_stations, time=fuse_data.time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.evaluation import spatial_rmse, spatial_r2, spatial_nse, spatial_bias\n",
    "from src.analysis.evaluation import temporal_rmse, temporal_r2, temporal_nse\n",
    "from src.analysis.evaluation import _nse_func, _rmse_func, _r2_func, _bias_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_func(preds_xr: xr.Dataset, error_str: str) -> pd.DataFrame:\n",
    "    lookup = {\n",
    "        \"nse\": _nse_func,\n",
    "        \"rmse\": _rmse_func,\n",
    "        \"r2\": _r2_func,\n",
    "        \"bias\": _bias_func,\n",
    "    }\n",
    "    error_func = lookup[error_str]\n",
    "    \n",
    "    df = preds_xr.to_dataframe()\n",
    "    df = df.dropna(how='any')\n",
    "    df = df.reset_index().set_index(\"time\")\n",
    "\n",
    "    station_ids = df[\"station_id\"].unique()\n",
    "    errors = []\n",
    "    for station_id in station_ids:\n",
    "        d = df.loc[df[\"station_id\"] == station_id]\n",
    "        if error_str == \"rmse\":\n",
    "            _error_calc = error_func(d[\"obs\"].values, d[\"sim\"].values, n_instances=d.size)\n",
    "        else:\n",
    "            _error_calc = error_func(d[\"obs\"].values, d[\"sim\"].values)\n",
    "        errors.append(_error_calc)\n",
    "\n",
    "    error = pd.DataFrame({\"station_id\": station_ids, error_str: errors})\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [ \n",
    "    error_func(ealstm_preds, \"nse\").set_index('station_id'),\n",
    "    error_func(ealstm_preds, \"r2\").set_index('station_id'), \n",
    "    error_func(ealstm_preds, \"rmse\").set_index('station_id'),\n",
    "    error_func(ealstm_preds, \"bias\").set_index('station_id'),\n",
    "]\n",
    "ealstm_df = errors[0].join(errors[1].join(errors[2]).join(errors[3])).reset_index()\n",
    "\n",
    "errors = [ \n",
    "    error_func(lstm_preds, \"nse\").set_index('station_id'),\n",
    "    error_func(lstm_preds, \"r2\").set_index('station_id'), \n",
    "    error_func(lstm_preds, \"rmse\").set_index('station_id'),\n",
    "    error_func(lstm_preds, \"bias\").set_index('station_id'),\n",
    "]\n",
    "lstm_df = errors[0].join(errors[1].join(errors[2]).join(errors[3])).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ealstm_nse = (\n",
    "    error_func(ealstm_preds, \"nse\")\n",
    "    .sort_values('nse')\n",
    "    .reset_index()\n",
    "    .drop('index', axis=1)\n",
    ")\n",
    "lstm_nse = (\n",
    "    error_func(lstm_preds, \"nse\")\n",
    "    .sort_values('nse')\n",
    "    .reset_index()\n",
    "    .drop('index', axis=1)\n",
    ")\n",
    "ealstm_nse['negative'] = ealstm_nse['nse'] < 0\n",
    "lstm_nse['negative'] = lstm_nse['nse'] < 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = fuse_data[\"obs\"].transpose(\"station_id\", \"time\")\n",
    "topmodel = fuse_data[\"SimQ_TOPMODEL\"]\n",
    "arnovic = fuse_data[\"SimQ_ARNOVIC\"]\n",
    "prms = fuse_data[\"SimQ_PRMS\"]\n",
    "sacramento = fuse_data[\"SimQ_SACRAMENTO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nse = spatial_nse(obs, topmodel).rename(\"TOPMODEL\")\n",
    "vic_nse = spatial_nse(obs, arnovic).rename(\"VIC\")\n",
    "prms_nse = spatial_nse(obs, prms).rename(\"PRMS\")\n",
    "sac_nse = spatial_nse(obs, sacramento).rename(\"Sacramento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse = xr.merge([\n",
    "    top_nse,\n",
    "    vic_nse,\n",
    "    prms_nse,\n",
    "    sac_nse,\n",
    "])\n",
    "nse_df = nse.to_dataframe()\n",
    "nse_df = static['gauge_name'].to_dataframe().join(nse_df).rename(columns=dict(gauge_name=\"Name\"))\n",
    "nse_df.to_csv(data_dir / 'gcloud/FUSE_nse_table.csv')\n",
    "nse_df.columns = [[\"nse\" for _ in range(len(nse_df.columns))], nse_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rmse = spatial_rmse(obs, topmodel).rename(\"TOPMODEL\")\n",
    "vic_rmse = spatial_rmse(obs, arnovic).rename(\"VIC\")\n",
    "prms_rmse = spatial_rmse(obs, prms).rename(\"PRMS\")\n",
    "sac_rmse = spatial_rmse(obs, sacramento).rename(\"Sacramento\")\n",
    "\n",
    "rmse = xr.merge([\n",
    "    top_rmse,\n",
    "    vic_rmse,\n",
    "    prms_rmse,\n",
    "    sac_rmse,\n",
    "])\n",
    "rmse_df = rmse.to_dataframe().drop(columns='time')\n",
    "rmse_df.to_csv(data_dir / 'gcloud/FUSE_rmse_table.csv')\n",
    "rmse_df.columns = [[\"rmse\" for _ in range(len(rmse_df.columns))], rmse_df.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_r2 = spatial_r2(obs, topmodel).rename(\"TOPMODEL\")\n",
    "vic_r2 = spatial_r2(obs, arnovic).rename(\"VIC\")\n",
    "prms_r2 = spatial_r2(obs, prms).rename(\"PRMS\")\n",
    "sac_r2 = spatial_r2(obs, sacramento).rename(\"Sacramento\")\n",
    "\n",
    "r2 = xr.merge([\n",
    "    top_r2,\n",
    "    vic_r2,\n",
    "    prms_r2,\n",
    "    sac_r2,\n",
    "])\n",
    "r2_df = r2.to_dataframe().drop(columns='time')\n",
    "r2_df.to_csv(data_dir / 'gcloud/FUSE_r2_table.csv')\n",
    "r2_df.columns = [[\"r2\" for _ in range(len(r2_df.columns))], r2_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_bias = spatial_bias(obs, topmodel).rename(\"TOPMODEL\")\n",
    "vic_bias = spatial_bias(obs, arnovic).rename(\"VIC\")\n",
    "prms_bias = spatial_bias(obs, prms).rename(\"PRMS\")\n",
    "sac_bias = spatial_bias(obs, sacramento).rename(\"Sacramento\")\n",
    "\n",
    "bias = xr.merge([\n",
    "    top_bias,\n",
    "    vic_bias,\n",
    "    prms_bias,\n",
    "    sac_bias,\n",
    "])\n",
    "bias_df = bias.to_dataframe()\n",
    "bias_df.to_csv(data_dir / 'gcloud/FUSE_bias_table.csv')\n",
    "bias_df.columns = [[\"bias\" for _ in range(len(bias_df.columns))], bias_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_errors = pd.concat([nse_df, rmse_df, r2_df, bias_df], axis=1)\n",
    "fuse_errors = fuse_errors.drop('time', axis=1, level=1).swaplevel(axis=1).sort_index(axis=1)\n",
    "fuse_errors.to_csv(data_dir / 'gcloud/FUSE_errors.csv')\n",
    "fuse_errors.to_pickle(data_dir / 'gcloud/FUSE_errors.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_nse_df = fuse_errors.drop(['bias', 'r2', 'rmse'], axis=1, level=1).droplevel(axis=1, level=1)\n",
    "fuse_bias = fuse_errors.drop(['nse', 'r2', 'rmse'], axis=1, level=1).droplevel(axis=1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.kdeplot(\n",
    "    lstm_nse['nse'], \n",
    "    cumulative=True, \n",
    "    legend=False, ax=ax, \n",
    "    clip=[-0.5,1], \n",
    "    label=f\"LSTM: {lstm_nse['negative'].sum()} stations with negative NSE\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ealstm_nse['nse'], \n",
    "    cumulative=True, \n",
    "    legend=False, ax=ax, \n",
    "    clip=[-0.5,1], \n",
    "    label=f\"EALSTM: {ealstm_nse['negative'].sum()} stations with negative NSE\"\n",
    ")\n",
    "\n",
    "for conceptual_ix, model in enumerate([c for c in fuse_nse_df.columns if (not \"Name\" in c) and (not \"station\" in c)]):\n",
    "    sns.kdeplot(\n",
    "        fuse_nse_df[model].dropna(), \n",
    "        cumulative=True, \n",
    "        legend=False, ax=ax, \n",
    "        clip=[-0.5,1], \n",
    "        label=f\"{model}: {(fuse_nse_df[model] < 0).sum()} stations with negative NSE\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    ax.axvline(fuse_nse_df[model].dropna().median(), ls=\"--\", color=colors[conceptual_ix+2])\n",
    "\n",
    "\n",
    "ax.axvline(lstm_nse['nse'].median(), ls=\"--\", color=colors[0])\n",
    "ax.axvline(ealstm_nse['nse'].median(), ls=\"--\", color=colors[1])\n",
    "\n",
    "ax.set_xlim([-0.2, 1])\n",
    "ax.set_xlabel(\"NSE\")\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "title = f\"Cumuluative Density Function of Station NSE Scores\"\n",
    "ax.set_title(title)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette()\n",
    "cumulative = True\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.kdeplot(\n",
    "    lstm_df['bias'], \n",
    "    cumulative=cumulative, \n",
    "    legend=False, ax=ax, \n",
    "    clip=[-50, 50], \n",
    "    label=f\"LSTM\"\n",
    ")\n",
    "sns.kdeplot(\n",
    "    ealstm_df['bias'],\n",
    "    cumulative=cumulative, \n",
    "    legend=False, ax=ax, \n",
    "    clip=[-50, 50], \n",
    "    label=f\"EALSTM\"\n",
    ")\n",
    "\n",
    "for conceptual_ix, model in enumerate([model for model in fuse_bias]):\n",
    "    sns.kdeplot(\n",
    "        fuse_bias[model].dropna(), \n",
    "        cumulative=cumulative, \n",
    "        legend=False, ax=ax, \n",
    "        label=f\"{model}\",\n",
    "        clip=[-50,50],\n",
    "    )\n",
    "    plt.legend()\n",
    "    ax.axvline(fuse_bias[model].dropna().mean(), ls=\"--\", color=colors[conceptual_ix+2])\n",
    "\n",
    "\n",
    "ax.axvline(lstm_df['bias'].mean(), ls=\"--\", color=colors[0])\n",
    "ax.axvline(ealstm_df['bias'].mean(), ls=\"--\", color=colors[1])\n",
    "\n",
    "ax.set_xlim([-30, 30])\n",
    "ax.set_xlabel(\"Bias\")\n",
    "\n",
    "if cumulative:\n",
    "    ax.set_ylabel(\"Cumulative density\")\n",
    "    title = f\"Cumuluative Density Function of Station Bias Scores\"\n",
    "else:\n",
    "    title = f\"Density of Station Bias Scores\"\n",
    "    ax.set_ylabel(\"Density\")\n",
    "# ax.axvline(ls=\"--\", color='grey', ) # label=\"Zero Bias\", \n",
    "ax.set_title(title)\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"LSTM Bias: {lstm_df['bias'].mean():.2f}\")\n",
    "print(f\"EALSTM Bias: {ealstm_df['bias'].mean():.2f}\")\n",
    "\n",
    "for model in [model for model in fuse_bias]:\n",
    "    print(f\"{model} Bias: {fuse_bias[model].dropna().mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_errors = pd.read_csv(data_dir / \"gcloud/jules_classic.csv\")\n",
    "\n",
    "classic = process_errors.loc[process_errors[\"Model\"] == \"Classic\", :].drop('Model', axis=1)\n",
    "classic = classic.rename(columns={\"ID\": \"Station ID\"}).set_index(\"Station ID\")\n",
    "classic.columns = [[\"CLASSIC\" for _ in range(len(classic.columns))], classic.columns]\n",
    "\n",
    "jules = process_errors.loc[process_errors[\"Model\"] == \"Jules\", :].drop('Model', axis=1)\n",
    "jules = jules.rename(columns={\"ID\": \"Station ID\"}).set_index(\"Station ID\").drop(\"Name\", axis=1)\n",
    "jules.columns = [[\"JULES\" for _ in range(len(jules.columns))], jules.columns]\n",
    "\n",
    "process_errors = pd.concat([classic, jules], axis=1)\n",
    "process_errors.to_pickle(data_dir / \"gcloud/process_models.pkl\")\n",
    "process_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_nse = fuse_errors.drop([\"rmse\", \"bias\", \"r2\"], axis=1, level=1).droplevel(axis=1, level=1)\n",
    "fuse_rmse = fuse_errors.drop([\"nse\", \"bias\", \"r2\"], axis=1, level=1).droplevel(axis=1, level=1)\n",
    "fuse_bias = fuse_errors.drop([\"rmse\", \"nse\", \"r2\"], axis=1, level=1).droplevel(axis=1, level=1)\n",
    "fuse_r2 = fuse_errors.drop([\"rmse\", \"bias\", \"nse\"], axis=1, level=1).droplevel(axis=1, level=1)\n",
    "\n",
    "fuse_nse = fuse_nse.join(fuse_errors.iloc[:, 0]).rename({(\"Name\", \"nse\"): 'Name'}, axis=1)\n",
    "fuse_rmse = fuse_rmse.join(fuse_errors.iloc[:, 0]).rename({(\"Name\", \"nse\"): 'Name'}, axis=1)\n",
    "fuse_bias = fuse_bias.join(fuse_errors.iloc[:, 0]).rename({(\"Name\", \"nse\"): 'Name'}, axis=1)\n",
    "fuse_r2 = fuse_r2.join(fuse_errors.iloc[:, 0]).rename({(\"Name\", \"nse\"): 'Name'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSE\n",
    "nse = fuse_nse_df.join(\n",
    "    ealstm_df.set_index(\"station_id\")[\"nse\"].rename(\"EALSTM\")\n",
    ")\n",
    "nse = nse.join(\n",
    "    lstm_df.set_index(\"station_id\")[\"nse\"].rename(\"LSTM\")\n",
    ")\n",
    "\n",
    "# RMSE\n",
    "rmse = fuse_rmse.join(\n",
    "    ealstm_df.set_index(\"station_id\")[\"rmse\"].rename(\"EALSTM\")\n",
    ")\n",
    "rmse = rmse.join(\n",
    "    lstm_df.set_index(\"station_id\")[\"rmse\"].rename(\"LSTM\")\n",
    ")\n",
    "rmse['Name'] = nse[\"Name\"]\n",
    "rmse = rmse[[\"Name\"] + [c for c in rmse.columns if c != \"Name\"]]\n",
    "\n",
    "# R2\n",
    "r2 = fuse_r2.join(\n",
    "    ealstm_df.set_index(\"station_id\")[\"r2\"].rename(\"EALSTM\")\n",
    ")\n",
    "r2 = r2.join(\n",
    "    lstm_df.set_index(\"station_id\")[\"r2\"].rename(\"LSTM\")\n",
    ")\n",
    "r2['Name'] = nse[\"Name\"]\n",
    "r2 = r2[[\"Name\"] + [c for c in r2.columns if c != \"Name\"]]\n",
    "\n",
    "\n",
    "# BIAS\n",
    "bias = fuse_bias.join(\n",
    "    ealstm_df.set_index(\"station_id\")[\"bias\"].rename(\"EALSTM\")\n",
    ")\n",
    "bias = bias.join(\n",
    "    lstm_df.set_index(\"station_id\")[\"bias\"].rename(\"LSTM\")\n",
    ")\n",
    "bias['Name'] = nse[\"Name\"]\n",
    "bias = bias[[\"Name\"] + [c for c in bias.columns if c != \"Name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nse_13 = nse.loc[catchment_ids]\n",
    "nse_13.index.name = \"Station ID\"\n",
    "rmse_13 = rmse.loc[catchment_ids]\n",
    "rmse_13.index.name = \"Station ID\"\n",
    "r2_13 = r2.loc[catchment_ids]\n",
    "r2_13.index.name = \"Station ID\"\n",
    "bias_13 = bias.loc[catchment_ids]\n",
    "bias_13.index.name = \"Station ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bias = bias_13.join(process_errors.drop([\"NSE\", \"Name\"], axis=1, level=1).droplevel(axis=1, level=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nse = nse_13.join(process_errors.drop([\"Bias\", \"Name\"], axis=1, level=1).droplevel(axis=1, level=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_nse.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_bias.to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuse_errors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vic_errors = fuse_errors.drop(['Name', 'PRMS', \"Sacramento\", \"TOPMODEL\"], axis=1, level=0).droplevel(axis=1, level=0)\n",
    "prms_errors = fuse_errors.drop(['Name', 'VIC', \"Sacramento\", \"TOPMODEL\"], axis=1, level=0).droplevel(axis=1, level=0)\n",
    "top_errors = fuse_errors.drop(['Name', 'PRMS', \"Sacramento\", \"VIC\"], axis=1, level=0).droplevel(axis=1, level=0)\n",
    "sac_errors = fuse_errors.drop(['Name', 'PRMS', \"VIC\", \"TOPMODEL\"], axis=1, level=0).droplevel(axis=1, level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "shp_path = Path(\n",
    "    \"/home/jovyan/runoff_uk_lstm/data/CAMELS/CAMELS_GB_DATASET/Catchment_Boundaries/CAMELS_GB_catchment_boundaries.shp\"\n",
    ")\n",
    "assert shp_path.exists()\n",
    "\n",
    "# load in the shapefile\n",
    "geo_df = gpd.read_file(shp_path)\n",
    "geo_df['ID_STRING'] = geo_df['ID_STRING'].astype('int')\n",
    "geo_df.crs = {'init' :'epsg:27700'}  # 4277  27700\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spatial dataframe\n",
    "assert lstm_df['station_id'].dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "lstm_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(lstm_df.set_index('station_id').join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "assert ealstm_df['station_id'].dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "ealstm_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(ealstm_df.set_index('station_id').join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "\n",
    "# CONCEPTUAL MODELS\n",
    "assert vic_errors.index.dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "vic_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(vic_errors.join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "assert prms_errors.index.dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "prms_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(prms_errors.join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "assert top_errors.index.dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "top_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(top_errors.join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "assert sac_errors.index.dtype == geo_df['ID_STRING'].dtype, \"Need to be the same type (integer)\"\n",
    "sac_gdf = gpd.GeoDataFrame(\n",
    "    geo_df.set_index('ID_STRING').join(sac_errors.join(static['p_mean'].to_dataframe()))\n",
    ")\n",
    "\n",
    "\n",
    "lstm_gdf.crs = {'init' :'epsg:27700'}\n",
    "ealstm_gdf.crs = {'init' :'epsg:27700'}\n",
    "vic_gdf.crs = {'init' :'epsg:27700'}\n",
    "prms_gdf.crs = {'init' :'epsg:27700'}\n",
    "top_gdf.crs = {'init' :'epsg:27700'}\n",
    "sac_gdf.crs = {'init' :'epsg:27700'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get UK Boundaries\n",
    "\n",
    "Get the COUNTY SHAPE data [here:](https://opendata.arcgis.com/datasets/1919db8ffcc5445ea4ba5b8a10acfccd_0.zip?outSR=%7B%22latestWkid%22%3A27700%2C%22wkid%22%3A27700%7D)\n",
    "```\n",
    "!wget https://opendata.arcgis.com/datasets/1919db8ffcc5445ea4ba5b8a10acfccd_0.zip\n",
    "!unzip 1919db8ffcc5445ea4ba5b8a10acfccd_0.zip\n",
    "!mkdir Counties_and_Unitary_Authorities_April_2019_Boundaries_EW_BFC\n",
    "!mv Counties* Counties_and_Unitary_Authorities_April_2019_Boundaries_EW_BFC\n",
    "```\n",
    "\n",
    "Get all of these shapefiles and merge into one big polygon\n",
    "```python\n",
    "uk = gpd.read_file(data_dir / \"gcloud/Counties_and_Unitary_Authorities_April_2019_Boundaries_EW_BFC/Counties_and_Unitary_Authorities_April_2019_Boundaries_EW_BFC.shp\")\n",
    "uk.plot()\n",
    "\n",
    "from shapely.ops import unary_union  # cascaded_union, \n",
    "uk_bound = unary_union([p for p in uk.geometry])\n",
    "uk_bound = gpd.GeoSeries(uk_bound)\n",
    "```\n",
    "\n",
    "[Link to CRS Discussion](https://communityhub.esriuk.com/geoxchange/2012/3/26/coordinate-systems-and-projections-for-beginners.html#:~:text=If%20you%20work%20with%20UK,that%20you%20should%20know%20about.&text=Web%20Mercator%20is%20a%20PCS,36%20used%20for%20British%20maps)\n",
    "\n",
    "\n",
    "![title](https://static1.squarespace.com/static/55bb8935e4b046642e9d3fa7/55bb8e8ee4b03fcc125a74c0/55bb8e91e4b03fcc125a7a67/1331725592717/1000w/coordsys_diagram.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = gpd.read_file(data_dir / \"gcloud/natural_earth_hires/ne_10m_admin_0_countries.shp\")\n",
    "uk = world.query(\"ADM0_A3 == 'GBR'\")\n",
    "# uk.plot(facecolor='none', edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"rmse\": {\"vmin\": 0, \"vmax\": 1, 'cmap': 'viridis'},   # rmse   \n",
    "    \"rmse_norm\": {\"vmin\": 0, \"vmax\": 0.5, 'cmap': 'viridis'},   # rmse   \n",
    "    \"nse\": {\"vmin\": 0, \"vmax\": 1, 'cmap': 'viridis_r'},    # nse   \n",
    "    \"r2\": {\"vmin\": 0, \"vmax\": 1, 'cmap': 'viridis_r'},    # r2  \n",
    "    \"bias\": {\"vmin\": -30, \"vmax\": 30, 'cmap': 'RdBu'}    # r2  \n",
    "}\n",
    "\n",
    "\n",
    "def plot_geospatial_data(model_data, model: str):\n",
    "    from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 8))\n",
    "    \n",
    "    for ix, metric in enumerate([\"rmse\", \"nse\", \"bias\"]):\n",
    "        ax = axs[ix]\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2) # depends on the user needs\n",
    "\n",
    "        # plot the surrounding lines\n",
    "        uk.plot(facecolor='none', edgecolor='k', ax=ax, linewidth=0.3)\n",
    "        # plot the chloropleth\n",
    "        model_data.to_crs(epsg=4326).plot(metric, ax=ax, legend=True, cax=cax, **opts[metric]);\n",
    "\n",
    "        ax.set_xlim([-8.2, 2.5])\n",
    "        ax.axis('off');\n",
    "\n",
    "        ax.set_title(metric.upper())\n",
    "\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.965])\n",
    "    fig.suptitle(f\"{model} Model Error\", size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geospatial_data(ealstm_gdf, model=\"EALSTM\")\n",
    "plot_geospatial_data(lstm_gdf, model=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geospatial_data(vic_gdf, model=\"VIC\")\n",
    "plot_geospatial_data(prms_gdf, model=\"PRMS\")\n",
    "plot_geospatial_data(top_gdf, model=\"TOPMODEL\")\n",
    "plot_geospatial_data(sac_gdf, model=\"Sacramento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
