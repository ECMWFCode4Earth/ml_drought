{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interpret static embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from ruamel.yaml import YAML\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = Path('data/')\n",
    "data_dir = Path('/cats/datastore/data/')\n",
    "\n",
    "assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ealstm_less_vars_2004_1707_1424', 'lstm_less_vars_2004_1507_1028', 'train_data.h5', 'train_data_scaler.p', 'lstm_ALL_vars_2004_2210_1035', 'lstm_all_vars_1998_2008_2210_110347', 'lstm_all_vars_1998_2008_2210_110727', 'lstm_all_vars_1998_2008_nh_2310_101443', 'lstm_all_vars_1998_2008_nh_2310_142625', 'ensemble__', 'ensemble_EALSTM', 'ensemble_lstm10', 'ensemble', '0_ensemble_results']\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/cats/datastore/data/runs/lstm_less_vars_2004_1507_1028')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([d.name for d in (data_dir / \"runs/\").iterdir()])\n",
    "print([d.name for d in (data_dir / \"runs/ealstm_less_vars_2004_1607_1334\").glob(\"*\")])\n",
    "\n",
    "(data_dir / \"runs/ealstm_less_vars_2004_1607_1334\")\n",
    "(data_dir / \"runs/lstm_less_vars_2004_1507_1028\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/tommy/tommy_multiple_forcing')\n",
    "\n",
    "\n",
    "from codebase.modelzoo.ealstm import EALSTM\n",
    "from codebase.modelzoo.cudalstm import CudaLSTM\n",
    "from codebase.data.camelstxt import CamelsGBCSV\n",
    "from codebase.data import get_h5_dataset\n",
    "from codebase.data.utils import load_basin_file\n",
    "from codebase.data.camelsh5 import CamelsGBH5\n",
    "from codebase.data.hdf5utils import create_h5_file\n",
    "from codebase.config import parse_config\n",
    "from codebase.errors import NoTrainDataError\n",
    "from codebase.training.regressiontrainer import RegressionTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/home/tommy/neural_hydrology')\n",
    "\n",
    "\n",
    "from codebase.modelzoo.ealstm import EALSTM\n",
    "from codebase.modelzoo.cudalstm import CudaLSTM\n",
    "from codebase.data.camelstxt import CamelsGBCSV\n",
    "from codebase.data import get_h5_dataset\n",
    "from codebase.data.utils import load_basin_file\n",
    "from codebase.data.camelsh5 import CamelsGBH5\n",
    "from codebase.data.hdf5utils import create_h5_file\n",
    "from codebase.config import parse_config\n",
    "from codebase.errors import NoTrainDataError\n",
    "from codebase.training.regressiontrainer import RegressionTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_file(run_dir: Path):\n",
    "    config_path = (run_dir / \"config.yml\") \n",
    "    yaml = YAML(typ=\"safe\")      \n",
    "    cfg = yaml.load(config_path)\n",
    "    cfg = parse_config(cfg)\n",
    "    \n",
    "    cfg[\"data_dir\"] = data_dir / \"CAMELS_GB_DATASET\"\n",
    "    cfg[\"scaler_file\"] = data_dir / \"runs/train_data_scaler.p\"\n",
    "    cfg[\"train_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "    cfg[\"validation_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "    cfg[\"test_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dir = data_dir / \"runs/ensemble_EALSTM/ealstm_ensemble6_nse_1998_2008_2910_030601\"\n",
    "\n",
    "# Config file\n",
    "config_path = (run_dir / \"config.yml\") \n",
    "yaml = YAML(typ=\"safe\")      \n",
    "cfg = yaml.load(config_path)\n",
    "cfg = parse_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data_dir / \"CAMELS_GB_DATASET\").exists()\n",
    "# assert (data_dir / \"runs/lstm_less_vars_2004_1507_1028/train_data/train_data_scaler.p\").exists()\n",
    "# assert (data_dir / \"runs/lstm_less_vars_2004_1507_1028/train_data/train_data.h5\").exists()\n",
    "assert (data_dir / \"runs/train_data_scaler.p\").exists()\n",
    "assert (data_dir / \"runs/train_data.h5\").exists()\n",
    "assert Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt').exists()\n",
    "\n",
    "# cfg[\"data_dir\"] = data_dir / \"CAMELS_GB_DATASET\"\n",
    "# cfg[\"scaler_file\"] = data_dir / \"runs/train_data_scaler.p\"\n",
    "# cfg[\"h5_file\"] = data_dir / \"runs/train_data.h5\"\n",
    "# cfg[\"train_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "# cfg[\"validation_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "# cfg[\"test_basin_file\"] = Path('/home/tommy/tommy_multiple_forcing/data/camels_gb_basin_list.txt')\n",
    "# cfg[\"target_variable\"] = \"discharge_spec\"\n",
    "# cfg[\"static_inputs\"] = []\n",
    "\n",
    "# run directory\n",
    "assert run_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new .h5 dataset and new .p scaler object\n",
    "if False:\n",
    "    fname = cfg[\"scaler_file\"].name\n",
    "    parents = cfg[\"scaler_file\"].parents[2]\n",
    "    new_scaler_file = parents / fname\n",
    "\n",
    "    fname = cfg[\"h5_file\"].name\n",
    "    parents = cfg[\"h5_file\"].parents[2]\n",
    "    new_h5_file = parents / fname\n",
    "\n",
    "    create_h5_file(\n",
    "        basins=basins,\n",
    "        cfg=cfg,\n",
    "        h5_file=new_h5_file,\n",
    "        scaler_file=new_scaler_file,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = RegressionTrainer(cfg)\n",
    "# trainer\n",
    "if False:\n",
    "    ds = get_h5_dataset(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    if not \"attributes\" in [k for k in globals().keys()]:\n",
    "        # load in the attributes (static data)\n",
    "        h5 = CamelsGBH5(cfg)\n",
    "        attributes = h5._load_attributes()\n",
    "        # means = attributes.mean()\n",
    "        # stds = attributes.std()\n",
    "    else:\n",
    "        pass\n",
    "    attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of basins that we actually have results for\n",
    "lstm_preds_df = pd.read_csv(\"/cats/datastore/data/runs/ensemble/data_ENS.csv\").drop(\"Unnamed: 0\", axis=1)\n",
    "lstm_preds_df[\"time\"] = pd.to_datetime(lstm_preds_df[\"time\"])\n",
    "lstm_preds = lstm_preds_df.set_index([\"station_id\", \"time\"]).to_xarray()\n",
    "\n",
    "VALID_BASINS = (lstm_preds_df[\"station_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the training data\n",
    "ds = xr.open_dataset(data_dir / \"RUNOFF/ALL_dynamic_ds.nc\")\n",
    "ds['station_id'] = ds['station_id'].astype(int)\n",
    "\n",
    "all_static = xr.open_dataset(data_dir / f'RUNOFF/interim/static/data.nc')\n",
    "all_static['station_id'] = all_static['station_id'].astype(int)\n",
    "static = all_static"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/cats/datastore/data/runs/ensemble_EALSTM/ealstm_ensemble6_nse_1998_2008_2910_030601/model_epoch030.pt')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(run_dir / \"model_epoch030.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.load(model_path, map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EALSTM:\n\tMissing key(s) in state_dict: \"weight_ih\", \"weight_hh\", \"bias\", \"input_net.weight\", \"input_net.bias\". \n\tUnexpected key(s) in state_dict: \"input_gate.weight\", \"input_gate.bias\", \"dynamic_gates.weight_ih\", \"dynamic_gates.weight_hh\", \"dynamic_gates.bias\". \n\tsize mismatch for head.net.0.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([14, 64]).\n\tsize mismatch for head.net.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([14]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-91c44ff9e160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEALSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"model_epoch030.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # extract weight and bias of input gate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m    847\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EALSTM:\n\tMissing key(s) in state_dict: \"weight_ih\", \"weight_hh\", \"bias\", \"input_net.weight\", \"input_net.bias\". \n\tUnexpected key(s) in state_dict: \"input_gate.weight\", \"input_gate.bias\", \"dynamic_gates.weight_ih\", \"dynamic_gates.weight_hh\", \"dynamic_gates.bias\". \n\tsize mismatch for head.net.0.weight: copying a param with shape torch.Size([1, 64]) from checkpoint, the shape in current model is torch.Size([14, 64]).\n\tsize mismatch for head.net.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([14])."
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = EALSTM(cfg)\n",
    "model_path = run_dir / \"model_epoch030.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "# # extract weight and bias of input gate\n",
    "# weight = model.lstm.weight_sh\n",
    "# bias = model.lstm.bias_s\n",
    "\n",
    "# # model information\n",
    "# input_size_dyn = len(cfg[\"dynamic_inputs\"])\n",
    "# input_size_stat = len(cfg[\"static_inputs\"] + cfg[\"camels_attributes\"])\n",
    "from torch.nn.modules.rnn import LSTM\n",
    "\n",
    "lstm_run_dir = data_dir / \"runs/ensemble/lstm_ensemble6_nse_1998_2008_2710_171032/\"\n",
    "cfg_lstm = load_config_file(lstm_run_dir)\n",
    "lstm_model = CudaLSTM(cfg)\n",
    "lstm_model.lstm = LSTM(25, 64)\n",
    "lstm_model_path = lstm_run_dir / \"model_epoch030.pt\"\n",
    "lstm_model.load_state_dict(torch.load(lstm_model_path, map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"EA LSTM Total Parameters:\", pytorch_total_params)\n",
    "# If you want to calculate only the trainable parameters:\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"EA LSTM Total Trainable Parameters:\", pytorch_total_params)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in lstm_model.parameters())\n",
    "print(\"LSTM Total Parameters:\", pytorch_total_params)\n",
    "# If you want to calculate only the trainable parameters:\n",
    "pytorch_total_params = sum(p.numel() for p in lstm_model.parameters() if p.requires_grad)\n",
    "print(\"LSTM Total Trainable Parameters:\", pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = load_basin_file(Path(cfg[\"train_basin_file\"]))\n",
    "# mode = train validation test\n",
    "mode = \"test\"\n",
    "# scaler \n",
    "with open(cfg[\"scaler_file\"], \"rb\") as fp:\n",
    "    scaler = pickle.load(fp)\n",
    "scaler[\"camels_attr_mean\"]\n",
    "\n",
    "EMBEDDING_FILE = config_path.parents[0] / \"EMBEDDINGS.pkl\"\n",
    "if not (EMBEDDING_FILE).exists():\n",
    "    # CALCULATE EMBEDDINGS\n",
    "    ignore_basins = []\n",
    "    all_embeddings = []\n",
    "    for basin in tqdm(basins):\n",
    "        try: \n",
    "            ds_test = CamelsGBCSV(\n",
    "                    basin=basin,\n",
    "                    cfg=cfg,\n",
    "                    mode=mode,\n",
    "                    scaler=scaler,\n",
    "            )\n",
    "\n",
    "            # get static weights/biases\n",
    "            static_weights, static_bias = [p for p in model.input_net.parameters()]\n",
    "\n",
    "            # CALCULATE the input gate embeddings\n",
    "            # input_gate = torch.sigmoid(torch.addmm(static_bias, ds_test.attributes, static_weights))\n",
    "            input_gate = torch.sigmoid(model.input_net(ds_test.attributes))\n",
    "            embedding = input_gate.detach().numpy()\n",
    "            all_embeddings.append(embedding)\n",
    "\n",
    "        except NoTrainDataError:\n",
    "            print(f\"{basin} has no training data\")\n",
    "            ignore_basins.append(basin)\n",
    "\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    embed_dict = {\"embeddings\": all_embeddings, \"missing_basins\": ignore_basins, \"all_basins\": basins}\n",
    "    with EMBEDDING_FILE.open(\"wb\") as fp:\n",
    "        pickle.dump(embed_dict, fp)\n",
    "    \n",
    "else:\n",
    "    # LOAD EMBEDDINGS\n",
    "    with EMBEDDING_FILE.open(\"rb\") as fp:\n",
    "        embed_dict = pickle.load(fp)\n",
    "        \n",
    "        \n",
    "all_embeddings = embed_dict[\"embeddings\"]\n",
    "ignore_basins = embed_dict[\"missing_basins\"]\n",
    "all_basins = embed_dict[\"all_basins\"]\n",
    "ORDER = [basin_id for basin_id in all_basins if basin_id not in ignore_basins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column corresponds to one basin and the rows denote the input gate value of the **`n hidden`** LSTM cells. The basins are ordered by ascending CAMELS GB gauge id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "img = ax.pcolor(all_embeddings.T, cmap='plasma')\n",
    "ax.set_xlabel(\"Basins\")\n",
    "ax.set_ylabel(\"Input gate neuron\")\n",
    "ax.set_title(\"Input gate activations\")\n",
    "cbar = plt.colorbar(img, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Activation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_by_static_feature(static_feature: str) -> np.ndarray:\n",
    "    feature_order = all_static.sel(station_id=[int(s) for s in ORDER])[[\"gauge_name\", static_feature]].sortby(static_feature).station_id.values\n",
    "    # create df of reordered index\n",
    "    d = pd.DataFrame({\"station_id\": ORDER})\n",
    "    d = d.reset_index().rename({\"index\": \"idx\"}, axis=1).set_index(\"station_id\")\n",
    "    reorder = d.loc[[str(si) for si in feature_order]].reset_index().idx.values\n",
    "    \n",
    "    return reorder\n",
    "\n",
    "reorder = order_by_static_feature(\"aridity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "img = ax.pcolor(all_embeddings.T[:, reorder], cmap='plasma')\n",
    "ax.set_xlabel(\"Basins\")\n",
    "ax.set_ylabel(\"Input gate neuron\")\n",
    "ax.set_title(\"Input gate activations: Ordered by Aridity\")\n",
    "cbar = plt.colorbar(img, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret Static Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.drafts.clusterutils import get_silhouette_scores, get_clusters, get_label_2_color, get_variance_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw attributes and normalise\n",
    "df_raw = static\n",
    "df_norm = (df_raw - df_raw.mean()) / df_raw.std()\n",
    "\n",
    "# DROP the stations with missing data\n",
    "df_norm = df_norm[np.isin(df_norm.index, VALID_BASINS)]\n",
    "raw_features = df_norm.values\n",
    "\n",
    "# Check that the order of basins is the same in embedding / static\n",
    "assert all([str(si) == ORDER[ix] for ix, si in enumerate(df_norm.index)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### silhouette scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (data_dir / \"RUNOFF/lstm_silhouette_scores_means.pkl\")\n",
    "f2 = (data_dir / \"RUNOFF/lstm_silhouette_scores_mins.pkl\")\n",
    "\n",
    "if (not f1.exists()) & (not f2.exists()):\n",
    "    print(\"==> Calculate mean/min silhouette scores for the EA-LSTM embedding\")\n",
    "    lstm_scores_means = defaultdict(list)\n",
    "    lstm_scores_mins = defaultdict(list)\n",
    "    \n",
    "    # for each basin embedding calc a distribution of \n",
    "    for lstm_embedding in tqdm(all_embeddings):\n",
    "        stacked_embedding = lstm_embedding.reshape(-1, 1)\n",
    "        mean_scores, min_scores = get_silhouette_scores(stacked_embedding)\n",
    "        for cluster, values in mean_scores.items():\n",
    "            lstm_scores_means[cluster].append(values)\n",
    "        for cluster, values in min_scores.items():\n",
    "            lstm_scores_mins[cluster].append(values)\n",
    "\n",
    "    with f1.open(\"wb\") as f:\n",
    "        pickle.dump(lstm_scores_means, f)\n",
    "    with f2.open(\"wb\") as f:\n",
    "        pickle.dump(lstm_scores_mins, f)\n",
    "\n",
    "else:\n",
    "    lstm_scores_means = pickle.load(f1.open(\"rb\"))\n",
    "    lstm_scores_mins = pickle.load(f2.open(\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (data_dir / \"RUNOFF/raw_silhouette_scores_means.pkl\")\n",
    "f2 = (data_dir / \"RUNOFF/raw_silhouette_scores_mins.pkl\")\n",
    "\n",
    "if (not f1.exists()) & (not f2.exists()):\n",
    "    print(\"==> Calculate mean/min silhouette scores for the raw catchment attributes\")\n",
    "    raw_scores_means, raw_scores_mins = get_silhouette_scores(raw_features)\n",
    "    with f1.open(\"wb\") as f:\n",
    "        pickle.dump(raw_scores_means, f)\n",
    "    with f2.open(\"wb\") as f:\n",
    "        pickle.dump(raw_scores_mins, f)\n",
    "else:\n",
    "    raw_scores_means = pickle.load(f1.open(\"rb\"))\n",
    "    raw_scores_mins = pickle.load(f2.open(\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "# plot \n",
    "ax[0].errorbar(x=list(lstm_scores_means.keys()), \n",
    "            y=[np.mean(vals) for vals in list(lstm_scores_means.values())],\n",
    "            yerr = [np.std(vals) for vals in list(lstm_scores_means.values())],\n",
    "            marker='o', \n",
    "            markevery=1,\n",
    "            label=\"LSTM embeddings\")\n",
    "\n",
    "# plot raw scores\n",
    "ax[0].plot(list(raw_scores_means.keys()), \n",
    "        list(raw_scores_means.values()), \n",
    "        marker='o', \n",
    "        markevery=1,\n",
    "        label=\"Raw catchment attributes\")\n",
    "\n",
    "ax[0].set_xlabel(\"Number of clusters\")\n",
    "ax[0].set_ylabel(\"Silhouette score\")\n",
    "ax[0].set_title(\"Mean silhoutte scores\")\n",
    "ax[0].legend()\n",
    "\n",
    "#\n",
    "ax[1].errorbar(x=list(lstm_scores_mins.keys()), \n",
    "            y=[np.mean(vals) for vals in list(lstm_scores_mins.values())],\n",
    "            yerr = [np.std(vals) for vals in list(lstm_scores_mins.values())],\n",
    "            marker='o', \n",
    "            markevery=1,\n",
    "            label=\"LSTM embeddings\")\n",
    "\n",
    "# plot raw scores\n",
    "ax[1].plot(list(raw_scores_mins.keys()), \n",
    "        list(raw_scores_mins.values()), \n",
    "        marker='o', \n",
    "        markevery=1,\n",
    "        label=\"Raw catchment attributes\")\n",
    "ax[1].set_xlabel(\"Number of clusters\")\n",
    "ax[1].set_ylabel(\"Silhouette score\")\n",
    "ax[1].set_title(\"Min. silhoutte scores\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "embeddings = dict(zip([int(s) for s in ORDER], all_embeddings))\n",
    "basins = [int(s) for s in ORDER]\n",
    "\n",
    "### DEFINE NUMBER OF CLUSTERS (pick 2) ###\n",
    "ks = [9, 10]\n",
    "# ---------------------------------------# \n",
    "\n",
    "clusters = {k: defaultdict(dict) for k in ks}\n",
    "for k in ks:\n",
    "    for name in ['lstm', 'raw']:\n",
    "        if name == \"lstm\":\n",
    "            features = np.array(list(embeddings.values()))\n",
    "        else:\n",
    "            features = raw_features\n",
    "        clusterer = KMeans(n_clusters=k, random_state=0, init='k-means++',\n",
    "                           n_init=200).fit(features)\n",
    "        for basin in basins:\n",
    "            if name == 'lstm':\n",
    "                emb = embeddings[basin]\n",
    "            else:\n",
    "                emb = df_norm.loc[df_norm.index == int(basin)].values\n",
    "            clusters[k][name][basin] = clusterer.predict(emb.reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoDataFrame of clusters\n",
    "all_s_df = all_static.sel(station_id=np.isin(all_static.station_id, static.index)).to_dataframe()\n",
    "gauge_lats = all_s_df.loc[:,\"gauge_lat\"]\n",
    "gauge_lons = all_s_df.loc[:,\"gauge_lon\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run KMEANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "\n",
    "clusters_dict = {}\n",
    "for n_cluster in clusters.keys():\n",
    "#     n_cluster = [k for k in clusters.keys()][0]\n",
    "\n",
    "    cluster_df = pd.DataFrame(\n",
    "        {\n",
    "            \"station_id\": [k for k in clusters[n_cluster][\"lstm\"].keys()], \n",
    "            \"lstm\": [k for k in clusters[n_cluster][\"lstm\"].values()],\n",
    "            \"raw\": [k for k in clusters[n_cluster][\"raw\"].values()],\n",
    "        }\n",
    "    )\n",
    "    cluster_df[\"clusters\"] = [n_cluster for _ in range(len(cluster_df))]\n",
    "    cluster_df = cluster_df.set_index(\"station_id\").join(gauge_lats).join(gauge_lons).reset_index()\n",
    "    cluster_gdf = gpd.GeoDataFrame(\n",
    "        cluster_df, geometry=[Point((lat, lon)) for lat, lon in zip(cluster_df[\"gauge_lat\"], cluster_df[\"gauge_lon\"])]\n",
    "    )\n",
    "    clusters_dict[n_cluster] = cluster_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT Spatial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD in the UK Hi-res borders\n",
    "assert (data_dir / \"RUNOFF/natural_earth_hires/ne_10m_admin_0_countries.shp\").exists(), \"Download the natural earth hires from https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\"\n",
    "\n",
    "world = gpd.read_file(data_dir / \"RUNOFF/natural_earth_hires/ne_10m_admin_0_countries.shp\")\n",
    "uk = world.query(\"ADM0_A3 == 'GBR'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import DefaultDict, Dict \n",
    "\n",
    "\n",
    "def get_label_2_color(cluster_df: pd.DataFrame) -> DefaultDict[str, Dict[int, str]]:\n",
    "    assert all(np.isin((\"lstm\", \"raw\", \"station_id\"), cluster_df.columns))\n",
    "    color_list = ['#1b9e77', '#d95f02', '#7570b3', '#e7298a', '#e6ab02', '#66a61e']\n",
    "    color_list = sns.color_palette()\n",
    "\n",
    "    # Get which basins are in each group\n",
    "    basin_in_cluster = defaultdict(dict)\n",
    "\n",
    "    lstm_groups = cluster_df.groupby(\"lstm\")['station_id'].apply(list).to_frame()\n",
    "    raw_groups = cluster_df.groupby(\"raw\")['station_id'].apply(list).to_frame()\n",
    "\n",
    "    # which cluster does the basin belong to (LSTM cluster)\n",
    "    for group, basins in lstm_groups.iterrows():\n",
    "        basin_in_cluster[\"lstm\"][group] = basins.values[0]\n",
    "    # which cluster does the basin belong to (raw cluster)\n",
    "    for group, basins in raw_groups.iterrows():\n",
    "        basin_in_cluster[\"raw\"][group] = basins.values[0]\n",
    "\n",
    "    # assign the same color to the clusters with max overlap\n",
    "    label_2_color = defaultdict(dict)\n",
    "\n",
    "    for label, basins in basin_in_cluster[\"lstm\"].items():\n",
    "        label_2_color[\"lstm\"][label] = color_list[label]\n",
    "\n",
    "        max_count = -1\n",
    "        color_label = None\n",
    "        for label2, basins2 in basin_in_cluster[\"raw\"].items():\n",
    "            intersect = set(basins).intersection(basins2)\n",
    "            if len(intersect) > max_count:\n",
    "                max_count = len(intersect)\n",
    "                color_label = label2\n",
    "\n",
    "        label_2_color[\"raw\"][color_label] = color_list[label]\n",
    "        \n",
    "    # assign the missing key/color\n",
    "    is_matching_keys = np.isin([k for k in label_2_color[\"lstm\"].keys()], [k for k in label_2_color[\"raw\"].keys()])\n",
    "    if not all(is_matching_keys):\n",
    "        is_missing_color = np.isin([k for k in label_2_color[\"lstm\"].values()], [k for k in label_2_color[\"raw\"].values()])\n",
    "        missing_keys = np.array([k for k in label_2_color[\"lstm\"].keys()])[~is_matching_keys]\n",
    "        missing_colors = np.array([k for k in label_2_color[\"lstm\"].values()])[~is_missing_color]\n",
    "        \n",
    "        for missing_key, missing_color in zip(missing_keys, missing_colors):\n",
    "            label_2_color[\"raw\"][missing_key] = str(missing_color)\n",
    "    \n",
    "    return label_2_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the maps\n",
    "attributes = static\n",
    "plot_outline = True\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,10))\n",
    "\n",
    "for i, k in enumerate(clusters.keys()):\n",
    "    cluster_gdf = clusters_dict[k]\n",
    "    label_2_color = get_label_2_color(cluster_gdf)\n",
    "    for j, name in enumerate(clusters[k].keys()):\n",
    "\n",
    "        data = defaultdict(list)\n",
    "        for basin, label in clusters[k][name].items():\n",
    "            data[\"lat\"].append(cluster_gdf.loc[cluster_gdf[\"station_id\"] == basin, \"gauge_lat\"].values)\n",
    "            data[\"lon\"].append(cluster_gdf.loc[cluster_gdf[\"station_id\"] == basin, \"gauge_lon\"].values)\n",
    "            data[\"color\"].append(label_2_color[name][label])\n",
    "\n",
    "\n",
    "        points = ax[j,i].scatter(x=data[\"lon\"],\n",
    "                                 y=data[\"lat\"],\n",
    "                                 c=data[\"color\"],\n",
    "                                 s=30, \n",
    "                                 zorder=2,\n",
    "                                 edgecolor='none', \n",
    "                                 linewidth=0.5)\n",
    "        \n",
    "        if plot_outline:\n",
    "            uk.plot(facecolor='none', edgecolor='k', ax=ax[j,i], linewidth=0.3)\n",
    "            # ax[j,i].set_xlim([-8.19, 2.5])\n",
    "            ax[j,i].set_xlim([-6.814, 1.854])\n",
    "            ax[j,i].set_ylim([49.7125, 59.5575])\n",
    "        \n",
    "        ax[j,i].axis(False)\n",
    "        if name == \"lstm\":\n",
    "            ax[j,i].set_title(f\"Using LSTM embeddings with k={k}\")\n",
    "        else:\n",
    "            ax[j,i].set_title(f\"Using raw catchment attributes with k={k}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Reduction\n",
    "> To test this, Fig. 10 shows the fractional reduction in variance of 13 hydrologic signatures due to clustering by both raw catchment attributes vs. by the EA-LSTM embed- ding layer. Ideally, the within-cluster variance of any partic- ular hydrological signature should be as small as possible, so that the fractional reduction in variance is as large (close to one) as possible. In both the k = 5 and k = 6 cluster exam- ples, clustering by the EA-LSTM embedding layer reduced variance in the hydrological signatures by more or approxi- mately the same amount as by clustering on the raw catch- ment attributes.\n",
    "\n",
    "> This indicates that the EA-LSTM embedding layer largely preserves the information content about hydrological behav- iors while overall increasing distinctions between groups of similar catchments. The EA-LSTM was able to learn about hydrologic similarity between catchments by directly train- ing on both catchment attributes and rainfall–runoff time se- ries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe containing the hyd. signatures per basin\n",
    "q_feats = ['q_mean', 'runoff_ratio', 'stream_elas', 'slope_fdc',\n",
    "    'baseflow_index', 'hfd_mean', 'Q5', 'Q95', 'high_q_freq', 'high_q_dur', 'low_q_freq',\n",
    "    'low_q_dur', 'zero_q_freq']\n",
    "\n",
    "df_q = all_static[q_feats].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the results\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "for i, k in enumerate(ks):\n",
    "    var_reduction = get_variance_reduction(clusters[k][\"lstm\"], clusters[k][\"raw\"], df_q)\n",
    "    raw_vals = pd.concat([s for s in var_reduction[\"raw\"].values()], axis=1).mean(axis=1)\n",
    "    lstm_vals = pd.concat([s for s in var_reduction[\"lstm\"].values()], axis=1).mean(axis=1)\n",
    "    ind = np.arange(len(raw_vals))  # the x locations for the groups\n",
    "    width = 0.35  # the width of the bars\n",
    "    rects2 = ax[i].bar(ind + width/2, 1 - lstm_vals.values, width, \n",
    "                    label='LSTM embeddings')\n",
    "    rects1 = ax[i].bar(ind - width/2, 1 - raw_vals.values, width,\n",
    "                    label='Raw catchment attributes')\n",
    "    \n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax[i].set_ylabel('Variance reduction')\n",
    "    ax[i].set_xticks(ind)\n",
    "    ax[i].set_xticklabels(raw_vals.index)\n",
    "    ax[i].tick_params(axis='x', rotation=90)\n",
    "    ax[i].set_title(f\"With k={k} clusters\")\n",
    "    ax[i].legend()\n",
    "    ax[i].set_ylim(0,0.75)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Visualisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted([d for d in all_static.data_vars]))\n",
    "all_static_df = all_static.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_seeds = 4  # 8 12 18\n",
    "seed = 12\n",
    "\n",
    "# Calculate UMAP embedding from the LSTM catchment embeddings\n",
    "transformer = umap.UMAP(n_neighbors=10, \n",
    "                        min_dist=0.1, \n",
    "                        n_components=2,\n",
    "                        random_state=seed).fit(all_embeddings)\n",
    "umap_embedding = transformer.transform(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(10,12))\n",
    "\n",
    "attribute_set = {'aridity': 'Aridity [-]', \n",
    "                 'elev_mean': 'Mean catchment elevation [m]', \n",
    "                 'p_mean': 'Mean daily precipitation [mm/day]', \n",
    "#                  'area': 'Catchment area [km2]', \n",
    "                 'hfd_mean': 'mean half-flow date [days since 1 Oct]', \n",
    "#                  'dpsbar': 'Mean Drainage Path Slope [m/km2]',\n",
    "                 \"urban_perc\": \"Urban Landcover Percentage\",\n",
    "#                  \"no_gw_perc\": \"Hydrogeology - rocks with no groundwater (%)\",\n",
    "                 \"reservoir_cap\": \"Reservoir Capacity [ML]\",\n",
    "                }\n",
    "for i, (attribute, title) in enumerate(attribute_set.items()):\n",
    "    m = i // 2\n",
    "    n = i % 2\n",
    "    norm = colors.Normalize(vmin=np.percentile(all_static_df[attribute], q=50) if attribute == \"reservoir_cap\" else all_static_df[attribute].min(), \n",
    "                            vmax=2 if attribute == 'aridity' else (np.percentile(all_static_df[attribute], q=50) if attribute == \"reservoir_cap\" else all_static_df[attribute].max()))\n",
    "    c = []\n",
    "    for basin in VALID_BASINS:\n",
    "        c.append(all_static_df.loc[all_static_df.index == basin, attribute].values[0])\n",
    "    \n",
    "    points = ax[m,n].scatter(umap_embedding[:,0], umap_embedding[:,1], c=c, norm=norm, label=\"Basins\")\n",
    "    \n",
    "    ax[m,n].set_title(title)\n",
    "    ax[m,n].set_xticks([], [])\n",
    "    ax[m,n].set_yticks([], [])\n",
    "    plt.colorbar(points, ax=ax[m,n])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [SKLEARN Silhouette Analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "- Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "- Silhouette coefficients (as these values are referred to as) near +1 indicate that the sample is far away from the neighboring clusters. A value of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters and negative values indicate that those samples might have been assigned to the wrong cluster.\n",
    "- Also from the thickness of the silhouette plot the cluster size can be visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "def run_multiple_cluster_comparisons(X):\n",
    "    \"\"\"\n",
    "    https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "    \"\"\"\n",
    "    range_n_clusters = np.arange(2, 15)\n",
    "\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_multiple_cluster_comparisons(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What features are we using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cfg[\"camels_attributes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cfg[\"dynamic_inputs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
