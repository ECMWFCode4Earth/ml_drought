{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml_drought\n",
    "\n",
    "This series of notebooks outlines how to use the pipeline created as part of the ECMWF Summer of Weather Code 2019. \n",
    "\n",
    "## The `environment` files\n",
    "The two environment files `environment.mac.yml`/`environment.ubuntu.cpu.yml` specify working conda environments for different platforms. In order to run the pipeline it is advised to install a new `conda` environment.\n",
    "\n",
    "## Pipeline Structure\n",
    "\n",
    "The pipeline is structured as below. We have a number of different classes, all written in the `src` directory. These have been tested with the tests written in the `tests` directory but with the same structure as the `src` directory. These tests can be a useful entry point to understand how we use each part of the pipeline\n",
    "\n",
    "- Exporters: `src/exporters`\n",
    "- Preprocessors: `src/preprocess`\n",
    "- Engineers: `src/engineer`\n",
    "- Models: `src/models`\n",
    "- Analysis: `src/analysis`\n",
    "\n",
    "<img src=\"img/pipeline_structure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporters\n",
    "\n",
    "The exporters work to download data from external sources. These sources vary and the methods for downloading data also vary. The exporters all inherit behaviour from the `BaseExporter` defined in `src/exporters/base.py`. The `SEAS5Exporter` and the `ERA5Exporter` both interact with the ECMWF / Copernicus [`cdsapi`](https://cds.climate.copernicus.eu/api-how-to). Other exporters work with ftp servers or websites.\n",
    "\n",
    "<img src=\"img/exporter_diagram.png\" style='background-color: #878787; border-radius: 25px; padding: 20px'>\n",
    "\n",
    "### Sources:\n",
    "- The `S5Exporter` and the `ERA5Exporter` work with the [`Climate Data Store` (CDS)](https://cds.climate.copernicus.eu/#!/home) to download data. \n",
    "- The `ERA5ExporterPOS` downloads data from the PlanetOS AWS data mirror which can be visualised [here](https://data.planetos.com/datasets/ecmwf_era5)\n",
    "- The `GLEAMExporter` downloads data from the [GLEAM FTP Server](https://www.gleam.eu/)\n",
    "- The `VHIExporter` downloads data from the [NOAA Vegetation Health FTP Server](https://www.star.nesdis.noaa.gov/smcd/emb/vci/VH/vh_ftp.php)\n",
    "- The `SRTMExporter` uses the [`elevation` package](https://github.com/bopen/elevation)\n",
    "\n",
    "NOTE: By default the data \n",
    "\n",
    "### Exporters API\n",
    "\n",
    "The exporters have a common `export` method which will download the data to the `data/raw` directory by default. If you wish to download the data elsewhere then you should provide an `pathlib.Path` path to the `Exporter`. \n",
    "\n",
    "**Be aware that data volumes are significant (can be upwards of 1TB if you use downloaded all data)**\n",
    "\n",
    "**NOTE: the area surrounding Kenya will be downloaded by default for the CDS Exporters. Otherwise data is global and is subset later**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tommylees/miniconda3/envs/crop/lib/python3.7/site-packages/xarray/core/merge.py:18: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n",
      "/Users/tommylees/miniconda3/envs/crop/lib/python3.7/site-packages/xarray/core/dataarray.py:1829: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  'DataArray', pd.Series, pd.DataFrame, pd.Panel]:\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if Path('.').absolute().parents[1].name == 'ml_drought':\n",
    "    os.chdir(Path('.').absolute().parents[1])\n",
    "\n",
    "from src import exporters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHIRPSExporter',\n",
       " 'ERA5Exporter',\n",
       " 'ERA5ExporterPOS',\n",
       " 'ESACCIExporter',\n",
       " 'GLEAMExporter',\n",
       " 'KenyaAdminExporter',\n",
       " 'S5Exporter',\n",
       " 'SRTMExporter']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(exporters)[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessors\n",
    "\n",
    "The preprocessors work to convert these different datasets into a unified data format. This makes testing and developing different models much more straightforward.\n",
    "\n",
    "There is a `Preprocessor` for each `Exporter`.\n",
    "\n",
    "These `Preprocessors` perform a number of tasks:\n",
    "- Put the data on a regular spatial grid\n",
    "- Put the data on a consistent temporal frequency (e.g. all data is converted to `monthly` timesteps)\n",
    "- Dimension names are standardized (`time, lat, lon`)\n",
    "- The same areal extend (by default Kenya is subset from the data).\n",
    "\n",
    "<img src=\"img/preprocess_diagram.png\" style='background-color: #878787; border-radius: 25px; padding: 20px'>\n",
    "\n",
    "The preprocessors offer an opportunity to tailor the pipeline to your own needs. You can easily change the area to be subset (the Region of Interest - ROI) for example.\n",
    "\n",
    "The preprocessors do a very useful task in making the data consistent. Working with a Unified Data Format is useful for many comparison tasks and is essential for training machine learning and staitistical models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHIRPSPreprocessor',\n",
       " 'ERA5MonthlyMeanPreprocessor',\n",
       " 'ESACCIPreprocessor',\n",
       " 'GLEAMPreprocessor',\n",
       " 'KenyaAdminPreprocessor',\n",
       " 'PlanetOSPreprocessor']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import preprocess\n",
    "\n",
    "dir(preprocess)[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessors API\n",
    "\n",
    "The main entry point to the preprocessors is through the `prerprocessor.preprocess()` function.\n",
    "\n",
    "Regridding the data requires you to have a reference `.nc` file that you want to use as the reference grid. This means that your data will be put onto the same `lat, lon` grid as the reference file.\n",
    "\n",
    "```python\n",
    "preprocessor.preprocess(\n",
    "    subset_str='kenya', \n",
    "    regrid=Path('path/to/reference/netcdf.nc')\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_preprocess_single',\n",
       " 'analysis',\n",
       " 'chop_roi',\n",
       " 'create_filename',\n",
       " 'data_folder',\n",
       " 'dataset',\n",
       " 'filter_outfiles',\n",
       " 'get_filepaths',\n",
       " 'interim',\n",
       " 'load_reference_grid',\n",
       " 'merge_files',\n",
       " 'out_dir',\n",
       " 'preprocess',\n",
       " 'preprocessed_folder',\n",
       " 'raw_folder',\n",
       " 'regrid',\n",
       " 'resample_time',\n",
       " 'static',\n",
       " 'static_vars']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = preprocess.ERA5MonthlyMeanPreprocessor()\n",
    "\n",
    "[method for method in dir(preprocessor) if '__' not in method]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer\n",
    "\n",
    "The Engineer is responsible for taking the `preprocessed` data from the `data/interim/*_preprocessed/` directories and writing to the `data/features` directory. \n",
    "\n",
    "In doing so the `Engineer` creates `train` and `test` data for different month-years. \n",
    "\n",
    "The label on the directory `data/features/{experiment}/{year}_{month}` (for example: `data/features/nowcast/2015_1`) refers to the `target` timestep. Therefore, our `y.nc` has the timestep `January 2015` in this example.\n",
    "\n",
    "<img src=\"img/engineer_diagram.png\" style='background-color: #878787; border-radius: 25px; padding: 20px'>\n",
    "\n",
    "### We currently have two `experiments` defined in the pipeline.\n",
    "\n",
    "These two experiments are accessed through the `Engineer` class as an argument - `experiment: str`.\n",
    "\n",
    "The **`OneMonthForecast`** experiment tries to predict the target variable next month. For example, we might use `total_preciptation` as our regressor (stored in `x.nc`) and want to predict vegetation health `VHI` stored in `y.nc`. \n",
    "\n",
    "We therefore use data for December 2014 (total_precipitation and VHI as an autoregressive component) to predict January 2015 VHI.\n",
    "\n",
    "The **`Nowcast`** experiment suggests that we have information about variables other than the target variable for the target time. So we have `total_preciptiation` information in January 2015 and we want to use that information to predict January 2015 `VHI`. This experiment is a good way of incorporating SEAS5 forecast data.\n",
    "\n",
    "- `x.nc` includes December 2014 `VHI` and `total_precipitation`, as well as January 2015 `total_precipitation` (non-target variable at target timestep).\n",
    "- `y.nc` contains January 2015 `VHI` - our target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The models are the implementation of machine learning methods for making predictions about our `target_variable`. \n",
    "\n",
    "We have currently implemented 5 models with varying levels of complexity. We have some simple baseline models (`parsimonious` models) such as `Persistence` but also some complex Neural Networks with architectures specific for hydro-meteorology ([`EARecurrentNetwork` (paper here)](https://arxiv.org/pdf/1907.08456.pdf))\n",
    "\n",
    "These classes work with data from the `data/features` directory and write predictions to the `data/models/{model}`. Results are stored in: `results.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['EARecurrentNetwork', 'LinearNetwork', 'LinearRegression', 'Persistence', 'RecurrentNetwork']"
      ],
      "text/plain": [
       "['EARecurrentNetwork',\n",
       " 'LinearNetwork',\n",
       " 'LinearRegression',\n",
       " 'Persistence',\n",
       " 'RecurrentNetwork']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src import models\n",
    "\n",
    "dir(models)[:3] + dir(models)[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EALSTM - `EARecurrentNetwork`\n",
    "\n",
    "Of particular interset is the Entity-Aware Long-Short Term Memory (EALSTM). This model was developed recently in a [paper looking at Regional Rainfall-Runoff modelling](https://arxiv.org/abs/1907.08456). \n",
    "\n",
    "The authors adapt the classical Long-Short Term Memory (LSTM) network architecture to include the input of static and dynamic data separately. The **static** data is passed to each cell of the EALSTM through the input gate, modifying the information from the **dynamic** data that enters the long-term memory component ($C$).\n",
    "\n",
    "In the diagram below we compare the cell architecture in the LSTM (*top*) and the EALSTM (*bottom*).\n",
    "\n",
    "<img src=\"img/ealstm_lstm.png\" style='background-color: #878787; border-radius: 25px; padding: 20px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The analysis directory contains code for interpreting the output of the models and for interrogating the input datasets. This is a very general directory with 'helper' code. \n",
    "\n",
    "Some of the tasks that the `src/analysis` directory can help with are:\n",
    "1. Subsetting your analysis by region.\n",
    "2. Subsetting your analysis by landcover types.\n",
    "2. Calculating various temporal aggregations (such as 3 Monthly moving averages).\n",
    "3. Comparisons of `true` against predictions `preds`.\n",
    "4. Calculating indices from results ([Vegetation Deficit Index used in other papers](https://www.mdpi.com/2072-4292/11/9/1099)).\n",
    "\n",
    "In order to see the pipeline analysis and outputs in action please checkout the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
